"""
Main training loop and evaluation logic for the ODONTO.IA project.
"""
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from tqdm import tqdm
import copy
import streamlit as st
from typing import List, Dict, Any, cast, Tuple, Sized

# Import advanced optimizers
from lion_pytorch import Lion
from pytorch_ranger import Ranger
# from sophia_optimizer import SophiaG

import config

# --- Data Augmentation Functions ---

def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(config.DEVICE)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion: nn.Module, pred: torch.Tensor, y_a: torch.Tensor, y_b: torch.Tensor, lam: float):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def cutmix_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(config.DEVICE)
    
    y_a, y_b = y, y[index]
    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size(), lam)
    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]
    # Adjust lambda to exactly match pixel ratio
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (x.size()[-1] * x.size()[-2]))
    
    return x, y_a, y_b, lam

def rand_bbox(size: torch.Size, lam: float) -> Tuple[int, int, int, int]:
    W = size[2]
    H = size[3]
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)

    # uniform
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2

def get_optimizer(model: nn.Module, optimizer_name: str, learning_rate: float, l2_lambda: float) -> optim.Optimizer:
    """Factory function for optimizers."""
    if optimizer_name == 'Adam':
        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'AdamW':
        return optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'SGD':
        return optim.SGD(model.parameters(), lr=learning_rate, weight_decay=l2_lambda, momentum=0.9)
    elif optimizer_name == 'Ranger':
        return Ranger(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)
    elif optimizer_name == 'Lion':
        return Lion(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)
    # elif optimizer_name == 'Sophia':
    #     return SophiaG(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)
    else:
        st.warning(f"Otimizador '{optimizer_name}' n√£o reconhecido. Usando Adam como padr√£o.")
        return optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_lambda)

def get_scheduler(optimizer: optim.Optimizer, scheduler_name: str, epochs: int, steps_per_epoch: int):
    """Factory function for learning rate schedulers."""
    if scheduler_name == 'CosineAnnealingLR':
        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    elif scheduler_name == 'OneCycleLR':
        return optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config.TRAINING_PARAMS['learning_rate'], 
                                             steps_per_epoch=steps_per_epoch, epochs=epochs)
    elif scheduler_name == 'Nenhum':
        return None
    else:
        st.warning(f"Agendador '{scheduler_name}' n√£o reconhecido. Nenhum agendador ser√° usado.")
        return None

def train_loop(model: nn.Module, train_loader: DataLoader, valid_loader: DataLoader, 
               criterion: nn.Module, optimizer: optim.Optimizer, scheduler, 
               epochs: int, patience: int, augmentation_technique: str) -> Dict[str, Any]:
    """The main training loop."""
    best_model_wts = copy.deepcopy(model.state_dict())
    best_val_loss = float('inf')
    epochs_no_improve = 0
    
    history = {
        "train_loss": [], "train_acc": [],
        "val_loss": [], "val_acc": []
    }
    
    progress_bar = st.progress(0)
    status_text = st.empty()

    train_dataset = train_loader.dataset
    valid_dataset = valid_loader.dataset

    if not (hasattr(train_dataset, '__len__') and hasattr(valid_dataset, '__len__')):
        st.error("O dataset de treino ou valida√ß√£o n√£o tem um m√©todo __len__ definido.")
        return {"weights": model.state_dict(), "history": history}

    train_dataset_size = len(cast(Sized, train_dataset))
    valid_dataset_size = len(cast(Sized, valid_dataset))

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        corrects_train = 0
        
        loop = tqdm(train_loader, desc=f"√âpoca {epoch+1}/{epochs}", leave=False)
        for inputs, labels in loop:
            inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)
            
            optimizer.zero_grad()

            if augmentation_technique in ['Mixup', 'Cutmix'] and model.training:
                if augmentation_technique == 'Mixup':
                    aug_inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, config.AUGMENTATION_PARAMS['mixup_alpha'])
                else: # Cutmix
                    aug_inputs, targets_a, targets_b, lam = cutmix_data(inputs, labels, config.AUGMENTATION_PARAMS['cutmix_alpha'])
                
                outputs = model(aug_inputs)
                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)
                # Accuracy for augmented data is tricky, so we calculate it on original-like data
                with torch.no_grad():
                    plain_outputs = model(inputs)
                    _, preds = torch.max(plain_outputs, 1)

            else: # Padr√£o
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)

            loss.backward()
            optimizer.step()
            
            running_loss += loss.item() * inputs.size(0)
            corrects_train += torch.sum(preds == labels.data)
            loop.set_postfix(loss=loss.item())

        epoch_loss = running_loss / train_dataset_size
        epoch_acc_train = float(corrects_train) / train_dataset_size
        history['train_loss'].append(epoch_loss)
        history['train_acc'].append(epoch_acc_train)
        
        # Validation phase
        model.eval()
        val_loss = 0.0
        corrects_val = 0
        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                corrects_val += torch.sum(preds == labels.data)
        
        val_loss /= valid_dataset_size
        val_acc = float(corrects_val) / valid_dataset_size
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        status_text.text(f"√âpoca {epoch+1}/{epochs} | Perda Treino: {epoch_loss:.4f} | Acur√°cia Treino: {epoch_acc_train:.4f} | Perda Val: {val_loss:.4f} | Acur√°cia Val: {val_acc:.4f}")
        
        if scheduler:
            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                scheduler.step(val_loss) # Monitorar a perda de valida√ß√£o
            else:
                scheduler.step()

        # Salvar o melhor modelo com base na menor perda de valida√ß√£o
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_model_wts = copy.deepcopy(model.state_dict())
            epochs_no_improve = 0
            st.toast(f"üéâ Novo melhor modelo encontrado na √©poca {epoch+1} com perda de valida√ß√£o: {val_loss:.4f}")
        else:
            epochs_no_improve += 1

        if epochs_no_improve >= patience:
            st.warning(f"Parada antecipada na √©poca {epoch+1} por falta de melhora na perda de valida√ß√£o.")
            break
            
        progress_bar.progress((epoch + 1) / epochs)

    return {"weights": best_model_wts, "history": history}

def compute_metrics(model: nn.Module, dataloader: DataLoader, classes: List[str]) -> Dict[str, Any]:
    """Computes and displays classification metrics, and returns the report dictionary."""
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
    report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True, zero_division=0)
    df_report = pd.DataFrame(report).transpose()
    st.dataframe(df_report)
    
    # Confusion Matrix
    cm = confusion_matrix(all_labels, all_preds)
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=ax)
    ax.set_xlabel('Predito')
    ax.set_ylabel('Verdadeiro')
    st.pyplot(fig)
    
    return cast(Dict[str, Any], report)

def error_analysis(model: nn.Module, dataloader: DataLoader, classes: List[str]):
    """Performs and displays an error analysis."""
    model.eval()
    misclassified = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(config.DEVICE), labels.to(config.DEVICE)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            
            for i in range(len(preds)):
                if preds[i] != labels[i]:
                    # Denormalize and convert to PIL for display
                    img_tensor = inputs[i].cpu()
                    img = np.transpose(img_tensor.numpy(), (1, 2, 0))
                    img = (img * np.array([0.229, 0.224, 0.225])) + np.array([0.485, 0.456, 0.406]) # Standard denormalization
                    img = np.clip(img, 0, 1)
                    
                    misclassified.append({
                        "image": img,
                        "true_label": classes[labels[i]],
                        "predicted_label": classes[preds[i]]
                    })
    
    if misclassified:
        st.write(f"Total de {len(misclassified)} imagens classificadas incorretamente.")
        
        # Display a few examples
        num_to_show = min(len(misclassified), 10)
        cols = st.columns(num_to_show)
        for i in range(num_to_show):
            with cols[i]:
                st.image(misclassified[i]['image'], width=100)
                st.caption(f"Verdadeiro: {misclassified[i]['true_label']}\nPredito: {misclassified[i]['predicted_label']}")
    else:
        st.success("Nenhuma imagem foi classificada incorretamente!")
